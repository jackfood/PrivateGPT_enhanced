PERSIST_DIRECTORY=db
MODEL_TYPE=LlamaCpp
MODEL_PATH=Models/llama-2-7b-chat.ggmlv3.q5_K_M.bin
EMBEDDINGS_MODEL_NAME=all-mpnet-base-v2
MODEL_N_CTX=1048
MODEL_N_THREADS=7
TARGET_SOURCE_CHUNKS=1
N_GPU_LAYERS=5
USE_MLOCK=1
N_BATCH=2048
PDFtotxt_Directory=D:/PrivateGPT/Python/~PrivateGPTx/Scripts/source_documents

MODEL_N_CTX_SUMMARY_LMM=1048
SUMMARY_MAXTOKEN_SIZE_SUMMARY_LMM=200
MODEL_PATH_SUMMARY_LMM=./models/llama-2-7b-chat.ggmlv3.q5_K_M.bin
INPUT_folder_SUMMARY_LLM=C:\\Users\\
OUTPUT_DIR_SUMMARY_LLM=C:\\Users\\
